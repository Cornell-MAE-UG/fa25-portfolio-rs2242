---
layout: project
title: Boeing 737 Ethical Analysis
description: MAE 4300: Engineers and Society
image: /assets/images/b737.webp
---

The Boeing 737 MAX crashes of Lion Air Flight 610 in 2018 and Ethiopian Airlines Flight 302 in 2019 are often explained as the result of a faulty flight control system called the Maneuvering Characteristics Augmentation System, or MCAS. While MCAS directly contributed to both accidents, focusing only on that system misses the larger ethical picture. These crashes were not caused by a single bad line of code or one isolated engineering mistake. They were the result of a series of interconnected technical, organizational, and regulatory decisions that gradually shifted priority away from safety. From an engineering ethics perspective, the Boeing 737 MAX program represents a systemic failure to uphold the responsibility engineers have to protect the public.

MCAS was added to the 737 MAX to address changes in the aircraft’s handling characteristics caused by larger engines. Its purpose was to automatically push the nose of the aircraft downward under certain conditions so the plane would feel similar to earlier 737 models. However, the system relied on input from only one angle of attack sensor and was capable of repeatedly trimming the aircraft nose down. This introduced a single point of failure into a system that had significant authority over the aircraft’s motion. In aerospace engineering, redundancy is a fundamental safety principle, especially for systems that can directly affect flight control. Designing a safety critical system that depends on a single sensor created a foreseeable risk. When that risk can lead to loss of control and loss of life, choosing not to add redundancy is not just a technical choice but an ethical one.

The ethical concerns surrounding MCAS were made worse by the lack of transparency provided to pilots. MCAS was not clearly explained in flight manuals, and pilots were not trained on how the system functioned or how aggressively it could respond. As a result, pilots were expected to diagnose and correct behavior caused by a system they did not know existed. This placed them at a severe disadvantage during already stressful and time critical situations. Engineers have a responsibility to ensure that operators understand the systems they rely on, especially when those systems can override pilot input. Withholding information that directly affects safety undermines informed decision making and removes an important layer of human oversight.

These design and communication issues did not occur in isolation. They were shaped by the broader organizational pressures surrounding the development of the 737 MAX. Boeing faced intense competition from Airbus, whose A320neo offered improved fuel efficiency while requiring little additional pilot training. In response, Boeing prioritized speed to market and maintaining a common type rating with earlier 737 aircraft. This goal strongly influenced engineering decisions, including efforts to minimize design changes that would trigger simulator training. Systems like MCAS were intentionally kept in the background to preserve the appearance that the aircraft handled the same as previous models. While cost and schedule constraints are a reality in engineering, allowing them to outweigh safety considerations crosses an ethical line. Engineers are expected to push back against pressures that compromise safety, even when those pressures come from within their own organization.

Regulatory oversight also played a role in enabling these failures. The Federal Aviation Administration uses a system in which manufacturers are allowed to perform parts of their own certification process. While this system is meant to improve efficiency, it creates conflicts of interest when manufacturers are under pressure to meet deadlines and control costs. In the case of the 737 MAX, critical aspects of MCAS were approved with limited independent review, and later changes to the system were not always reevaluated with the level of scrutiny they deserved. When regulators rely too heavily on the companies they are meant to oversee, accountability becomes unclear and safety concerns can be overlooked.

This case also highlights differing definitions of what safety means. For Boeing, safety was largely treated as meeting regulatory requirements and preserving certification continuity. For pilots and passengers, safety meant predictable aircraft behavior, clear communication, and proper training to respond to emergencies. Ethical engineering requires going beyond minimum compliance and considering how systems will actually be used in real world conditions. In the 737 MAX program, regulatory compliance became the goal rather than the baseline, which allowed serious risks to remain unaddressed.

The Boeing 737 MAX crashes were not the result of one mistake, but of a system that allowed unsafe decisions to accumulate across multiple levels. Technical design choices, organizational incentives, lack of transparency, and weak oversight reinforced one another until failure became inevitable. At many points, engineers, managers, and regulators had opportunities to question assumptions and prioritize safety. The failure to do so reflects a breakdown in ethical responsibility. The lesson of the 737 MAX is that engineering ethics are not optional or secondary to business goals. Protecting human life must remain the central obligation of the engineering profession, even when it conflicts with cost, schedule, or competitive pressure.